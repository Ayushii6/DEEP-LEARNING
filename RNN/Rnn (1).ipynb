{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec3d28a1-327c-4d1e-b69c-b86ab754ddb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libraries imported ðŸ˜Ž\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer ## stemming \n",
    "from nltk.stem import WordNetLemmatizer  ## lemmatization\n",
    "import joblib,os \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences \n",
    "from sklearn.preprocessing import LabelEncoder ,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('libraries imported ðŸ˜Ž')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "349645d0-6f25-460c-b7e2-b12622a69268",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading data from the text files \n",
    "\n",
    "train_data = open(\"./train.txt\").readlines()\n",
    "val_data = open(\"./val.txt\").readlines()\n",
    "test_data = open(\"./test.txt\").readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a986b3ce-8dcd-4c90-ba19-3962d8ae3de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03fa62f9-f16b-4f72-b304-bff76bdf2355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "complet_data = train_data + test_data + val_data \n",
    "print(len(complet_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a524209-6a55-4844-afa9-51cd9842344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "for temp in complet_data:\n",
    "    complete = temp.split(';')\n",
    "    if len(complete)==2:\n",
    "        x.append(complete[0])\n",
    "        y.append(complete[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ace463b-dac6-48b5-a4d5-fdd63946b80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sadness', 'anger', 'love', 'surprise', 'fear', 'joy']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=[]\n",
    "for item in y:\n",
    "        if item not in labels:\n",
    "            labels.append(item)\n",
    "labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0605f5cc-53fa-494a-8548-0d8d6b6b8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text cleaning \n",
    "# 1. lower case convert all message \n",
    "# 2. a-z0-9   other characters \n",
    "# 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec0144-333d-48b1-a014-e9ad361a2161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89693722-a988-4a59-8fb1-7c6897a64dd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\SKTW/nltk_data'\n    - 'c:\\\\Users\\\\SKTW\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\SKTW\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\SKTW\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\SKTW\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cleaned_data\n\u001b[0;32m     14\u001b[0m stem \u001b[38;5;241m=\u001b[39m PorterStemmer()\n\u001b[1;32m---> 15\u001b[0m cleaned_data \u001b[38;5;241m=\u001b[39m \u001b[43mtext_cleaning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstemming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m, in \u001b[0;36mtext_cleaning\u001b[1;34m(sentences, stemming)\u001b[0m\n\u001b[0;32m      5\u001b[0m message \u001b[38;5;241m=\u001b[39m sentece\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m      6\u001b[0m message \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-z0-9 ]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,message)\n\u001b[1;32m----> 7\u001b[0m ls_of_words \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m ls_of_word_without_stopwords \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m ls_of_words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m      9\u001b[0m stemmed_words \u001b[38;5;241m=\u001b[39m [stemming\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m ls_of_word_without_stopwords ]\n",
      "File \u001b[1;32mc:\\Users\\SKTW\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\SKTW\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\SKTW\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\SKTW\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Users\\SKTW\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\SKTW/nltk_data'\n    - 'c:\\\\Users\\\\SKTW\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n    - 'c:\\\\Users\\\\SKTW\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\SKTW\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\SKTW\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# text cleaning function applying on each and every raw message \n",
    "def text_cleaning(sentences,stemming):\n",
    "    cleaned_data = []\n",
    "    for sentece in sentences: \n",
    "        message = sentece.lower()\n",
    "        message = re.sub('[^a-z0-9 ]',\"\",message)\n",
    "        ls_of_words = nltk.word_tokenize(message)\n",
    "        ls_of_word_without_stopwords = [word for word in ls_of_words if word not in stopwords.words('english')]\n",
    "        stemmed_words = [stemming.stem(word) for word in ls_of_word_without_stopwords ]\n",
    "        message = \" \".join(stemmed_words)\n",
    "        cleaned_data.append(message)\n",
    "    return cleaned_data\n",
    "\n",
    "stem = PorterStemmer()\n",
    "cleaned_data = text_cleaning(x,stemming=stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d6e6ff-eaa9-4e10-906c-fdc8deb4b54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully saved your cleaned data!\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('models',exist_ok=True)\n",
    "joblib.dump(cleaned_data,\"./models/cleaned_data.lb\")\n",
    "print(\"successfully saved your cleaned data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5a53e7-53df-4028-a484-2ae58bb2cd9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e92db-b4b2-4711-9355-59f023353ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake\n",
      "['go', 'feeling', 'hopeless', 'damned', 'hopeful', 'around', 'someone', 'cares', 'awake']\n",
      "['go', 'feel', 'hopeless', 'damn', 'hope', 'around', 'someon', 'care', 'awak']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'go feel hopeless damn hope around someon care awak'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## trying  on a sinlgle message \n",
    "ls_of_stopwords = stopwords.words('english')\n",
    "print(x[1])\n",
    "single_message = x[1].lower()\n",
    "single_message = re.sub('[^a-z0-9 ]',\"\",single_message)\n",
    "ls_of_words = nltk.word_tokenize(single_message)  # single_message.split()\n",
    "ls_of_word_without_stopwords = []\n",
    "for word in ls_of_words: \n",
    "    if word not in ls_of_stopwords:\n",
    "        ls_of_word_without_stopwords.append(word)\n",
    "print(ls_of_word_without_stopwords)\n",
    "stem = PorterStemmer()\n",
    "stemmed_words = []\n",
    "for word in ls_of_word_without_stopwords: \n",
    "    stemmed_words.append(stem.stem(word))\n",
    "print(stemmed_words)\n",
    "\" \".join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab042279-feb3-4d6e-8735-60262a2ad641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fc2d12-b41e-4d98-9089-606ae0d4db91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully saved your tokenizer at this location : './models/tokenizer.lb'\n"
     ]
    }
   ],
   "source": [
    "## Tokenaization \n",
    "tokenizer = Tokenizer(oov_token=\"<nothing>\")\n",
    "tokenizer.fit_on_texts(cleaned_data)\n",
    "joblib.dump(tokenizer,'./models/tokenizer.lb')\n",
    "print(\"successfully saved your tokenizer at this location : './models/tokenizer.lb'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6c3c72-61a2-4258-9a58-0cc590d2dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef78b5-93c2-47b0-91b1-f3cc41855ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_counts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025611d3-8a94-401a-a5dd-44d6a09999a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenizer.texts_to_sequences(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5011cbdd-5574-450f-92f4-682f280917e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7151c8f6-4f10-42c4-b6c8-e24c58290d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'didnt feel humili'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03fc307-c490-4a79-ba01-bbc56b23c312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61, 2, 522]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9946d07c-9490-4ea5-8bb6-35363e6308a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8273e59-a5d1-444e-8c72-8cf1a7634d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your maximum length is :  35\n"
     ]
    }
   ],
   "source": [
    "len_of_message = []\n",
    "for i in range(len(tokenized_data)):\n",
    "    len_of_message.append(len(tokenized_data[i]))\n",
    "print(\"Your maximum length is : \",max(len_of_message))\n",
    "# maximum length  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b653d-1480-4a4b-9257-2ff0749dc4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your maximum length is :  35\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Your maximum length is : \",max(list(map(len,tokenized_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8117bfbe-fe02-468b-9e15-6e981bbe95f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pad_sequences(tokenized_data,maxlen=35,padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca60da-abfd-4e31-bd46-c7eecc6062f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  61,    2,  522, ...,    0,    0,    0],\n",
       "       [  10,    2,  419, ...,    0,    0,    0],\n",
       "       [   4, 1230,  431, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   2,  194,  157, ...,    0,    0,    0],\n",
       "       [ 328,    2,  175, ...,    0,    0,    0],\n",
       "       [   2,    3,  916, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10dfc98-9e4e-4039-9931-1dd3141b6482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sadness': 0, 'anger': 1, 'love': 2, 'surprise': 3, 'fear': 4, 'joy': 5}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=[]\n",
    "for item in y:\n",
    "        if item not in labels:\n",
    "            labels.append(item)\n",
    "label_dict = {label:i for i , label in enumerate(labels)}\n",
    "label_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff6cdb4-bce0-4e24-aa5c-4d45b835ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder ,OneHotEncoder\n",
    "# automation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80814ada-fe8b-4016-86f3-ef6a2fba3c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "Y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2f31de-645f-4686-af5f-1d617bd89c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 0, ..., 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0788f-5c6c-47b4-b577-d09264de3d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'fear', 'joy', 'love', 'sadness', 'surprise'], dtype='<U8')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447bbcab-c380-4da1-a3c0-921d0a6be1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/label_encoder.lb']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(label_encoder,'./models/label_encoder.lb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab001c7-8187-4c1f-ac0d-fde170e34ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 0, ..., 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences    # <====  X DATA  cleaned data \n",
    "Y            # <====  Y DATA label \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86d0fb1-36af-4a93-b7a7-42fe7cdf6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train test splitting \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(sequences,Y,test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c700cbf-303d-486d-b4bc-fa80ea2b755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model training\n",
    "## Model define  \n",
    "# ANN , CNN , RNN  \n",
    "### logistic regression ,\n",
    "### DTC , RDC , \n",
    "## machine learning algorithm   | ANN   | RNN \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db387589-4eaa-403a-95ee-1f706afee499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2a300-0f83-4e82-9b7b-c523cc438c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e733804b-b5bb-4d8b-a25d-17fdc749177f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65faa4d3-5115-481c-8bb5-02a3acf3140c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
